{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab06_tree-methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6 - Tree-based Methods\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "-------------------\n",
        "\n",
        "In today's class, we will explore **Classification and Regression Trees (CART)** and the **Random Forest** algorithm, both of which represent **tree-based methods** in machine learning. **CART** serves as a foundational algorithm capable of handling both **classification** and **regression** tasks. It works by recursively partitioning the data based on the most informative features, resulting in a simple yet powerful binary decision tree. Using measures like **Gini impurity** for classification and **mean squared error** for regression, **CART** selects the best splits to optimize predictive accuracy.\n",
        "\n",
        "While individual decision trees are easy to interpret, they can suffer from **overfitting**, limiting their generalization to new data. This challenge is effectively addressed by **Random Forest**, an **ensemble method** that constructs multiple decision trees on random subsets of the data and aggregates their predictions. By averaging results in regression tasks or using majority voting in classification, **Random Forest** significantly improves accuracy and robustness while reducing overfitting.\n",
        "\n",
        "Both **CART** and **Random Forest** are considered **off-the-shelf** methods, meaning they can be applied directly to a wide range of problems with minimal tuning, making them go-to solutions for many real-world machine learning tasks. Today, we will implement these algorithms, apply them to datasets, and evaluate their performance to better understand their practical applications."
      ],
      "metadata": {
        "id": "nbnkQ4BV2_fJ"
      },
      "id": "nbnkQ4BV2_fJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CART: Detailed Explanation\n",
        "----------------------------"
      ],
      "metadata": {
        "id": "5wJDCVb375yq"
      },
      "id": "5wJDCVb375yq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Split into Regions in Decision Trees\n",
        "\n",
        "In both **classification** and **regression trees**, the data space is recursively partitioned into rectangular regions based on feature values. At each node of the tree, the algorithm selects a **feature** and a **threshold** that best splits the data into two subsets. This process continues recursively, resulting in a hierarchical partitioning of the feature space.\n",
        "\n",
        "- Each split corresponds to a decision rule, like $X_j < t$, where $X_j$ is a feature and $t$ is the threshold.\n",
        "- The data points that satisfy the rule go to the left branch; the rest go to the right.\n",
        "- The process continues until a stopping criterion is met (e.g., maximum depth, minimum number of samples, or impurity threshold).\n",
        "\n",
        "The end result is a division of the space into **non-overlapping regions** $R_1, R_2, \\dots, R_M$, where each region corresponds to a terminal (leaf) node in the tree.\n"
      ],
      "metadata": {
        "id": "se4dx0G179YH"
      },
      "id": "se4dx0G179YH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## How the Regression Tree Predicts a New Value\n",
        "\n",
        "In a **regression tree**, the prediction for a new observation is based on the **mean** of the target values in the region where the observation falls.\n",
        "\n",
        "- When a new data point is passed through the tree, it follows the decision rules from the root to a specific leaf node.\n",
        "- The predicted value is the **average** of the training data points within that leaf's region.\n",
        "\n",
        "**Mathematically:**\n",
        "\n",
        "If a region $R_m$ contains data points $\\{y_i\\}_{i=1}^{N_m}$, the prediction $\\hat{y}$ for any $x \\in R_m$ is:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\frac{1}{N_m} \\sum_{i: x_i \\in R_m} y_i\n",
        "$$"
      ],
      "metadata": {
        "id": "LGhJOCQq7_gY"
      },
      "id": "LGhJOCQq7_gY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How the Regression Tree Builds the Region Partitioning\n",
        "\n",
        "The goal in regression is to minimize the **sum of squared residuals (SSR)** within each region. At each split, the algorithm selects:\n",
        "\n",
        "- The **feature** $X_j$ to split on.\n",
        "- The corresponding **threshold** $t$ for that feature.\n",
        "\n",
        "**The selection process involves both $X_j$ (the feature) and $t$ (the split point).**\n",
        "\n",
        "The algorithm proceeds as follows:\n",
        "\n",
        "1. **For each feature** $X_j$:\n",
        "   - Consider all possible thresholds $t$ (often midpoints between sorted unique values).\n",
        "   - Evaluate the SSR for each possible split.\n",
        "\n",
        "2. **Select the feature $X_j^*$ and threshold $t^*$** that minimize the total SSR:\n",
        "\n",
        "  $$\n",
        "  \\sum_{m=1}^{M} \\sum_{i: x_i \\in R_m} (y_i - \\bar{y}_{R_m})^2\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "\n",
        "  - $R_m$ is a region (leaf node) defined by the splits.\n",
        "  - $\\bar{y}_{R_m}$ is the mean target value in region $R_m$.\n",
        "\n",
        "  This process is repeated recursively for each new subset until a stopping criterion is met."
      ],
      "metadata": {
        "id": "bkjR4WoB8HMX"
      },
      "id": "bkjR4WoB8HMX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How the Classification Tree Predicts a Class for a New Observation\n",
        "\n",
        "In a **classification tree**, the prediction for a new observation is based on the **majority class** within the region it falls into.\n",
        "\n",
        "- The new observation follows the decision rules down the tree until it reaches a leaf node.\n",
        "- The predicted class is the one with the highest proportion of samples in that leaf.\n",
        "\n",
        "If region $R_m$ contains samples from classes $C_1, C_2, \\dots, C_k$, the predicted class $\\hat{C}$ is:\n",
        "\n",
        "$$\n",
        "\\hat{C} = \\arg\\max_{c} \\, P_c\n",
        "$$\n",
        "\n",
        "Where $P_c$ is the proportion of class $c$ in region $R_m$."
      ],
      "metadata": {
        "id": "LxkEBdGH8I-6"
      },
      "id": "LxkEBdGH8I-6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How the Classification Tree Builds the Region Partitioning\n",
        "\n",
        "In classification, the goal is to minimize impurity in the resulting regions. At each split, the algorithm selects:\n",
        "\n",
        "- The **feature** $X_j$ to split on.\n",
        "- The corresponding **threshold** $t$ for that feature.\n",
        "\n",
        "**The selection process again involves both $X_j$ (the feature) and $t$ (the split point).**\n",
        "\n",
        "The process is as follows:\n",
        "\n",
        "1. **For each feature** $X_j$:\n",
        "   - Consider all possible thresholds $t$.\n",
        "   - Calculate impurity measures (Gini or Cross-Entropy) for the resulting splits.\n",
        "\n",
        "2. **Select the feature $X_j^*$ and threshold $t^*$** that result in the **largest decrease in impurity**.\n",
        "\n",
        "  a) Gini Index\n",
        "\n",
        "    The **Gini Index** measures the probability of misclassification:\n",
        "\n",
        "    $$\n",
        "    G(R_m) = 1 - \\sum_{c=1}^{K} p_{mc}^2\n",
        "    $$\n",
        "\n",
        "    Where:\n",
        "\n",
        "    - $p_{mc}$ is the proportion of samples of class $c$ in region $R_m$.\n",
        "\n",
        "  b) Cross-Entropy (Deviance)\n",
        "\n",
        "    The **Cross-Entropy** measure is:\n",
        "\n",
        "    $$\n",
        "    H(R_m) = - \\sum_{c=1}^{K} p_{mc} \\log(p_{mc})\n",
        "    $$\n",
        "\n",
        "The split that leads to the **greatest reduction in impurity** (using either Gini or Cross-Entropy) is selected.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p7AQhpqe7crc"
      },
      "id": "p7AQhpqe7crc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Key Points\n",
        "\n",
        "- The **tree-building process selects both the feature** $X_j$ **and the threshold** $t$ **that optimally split the data** based on the chosen objective (SSR for regression, impurity reduction for classification).\n",
        "- This process continues recursively, resulting in a tree that partitions the data space into regions with either low variance (for regression) or low impurity (for classification).\n"
      ],
      "metadata": {
        "id": "JdoKXPuS8MC7"
      },
      "id": "JdoKXPuS8MC7"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}