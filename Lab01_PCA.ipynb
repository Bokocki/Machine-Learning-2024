{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab01_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68688b1a",
      "metadata": {
        "id": "68688b1a"
      },
      "source": [
        "# Lab1 - PCA\n",
        "### Author: Szymon Nowakowski\n",
        "\n",
        "In this lab, we will conduct Principal Component Analysis (PCA) to identify a direction in feature space (a linear combination of features) that captures the maximum variance in the data.\n",
        "\n",
        "We’ll explore the use of Singular Value Decomposition (SVD) as an efficient technique for performing PCA.\n",
        "\n",
        "Next, we’ll implement Python code to demonstrate PCA and its applications..\n",
        "\n",
        "# Motivation\n",
        "\n",
        "## Data\n",
        "\n",
        "Let's assume $X$ is an $n \\times k$ matrix with our **data**. Let's further assume it is **centered**.\n",
        "\n",
        "- Rows of $X$ are **observations** or **data points**, while\n",
        "- Columns of $X$ are called **features**.\n",
        "\n",
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\dots & x_{1k} \\\\\n",
        "x_{21} & x_{22} & \\dots & x_{2k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{n1} & x_{n2} & \\dots & x_{nk}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## PCA\n",
        "\n",
        "In PCA, our goal is to find a direction (a linear combination of features) that captures the maximum variance in the data.\n",
        "\n",
        "- **Least Scattered View Along the Principal Component**: this direction can be understood as the direction along which the data points appear least scattered when viewed from that angle.\n",
        "\n",
        "- **Direction of Maximum Variance**: alternatively—and equivalently, due to the Pythagorean theorem—when we project the data onto this direction, the spread (or variance) of the data is maximized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our goal\n",
        "\n",
        "Our goal is finding such a $w$, that the projection's $X w$ variance is maximized:\n",
        "\n",
        "$$\n",
        "\\text{Var}(X w) \\longrightarrow \\max_{\\|w\\| = 1}\n",
        "$$\n",
        "\n",
        "Obviously we restrict $w$ to normalized vectors.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OtoYGLdPNwYD"
      },
      "id": "OtoYGLdPNwYD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inner Product as Projection Length\n",
        "\n",
        "When we take the inner product of a vector $x$ with a normalized vector $w$, we’re effectively measuring its \"shadow\" or \"projection\" onto\n",
        "$w$. The inner product is a scalar value. This scalar represents the length of\n",
        "$x$ along the direction of $w$—essentially, how far $x$ extends in the\n",
        "$w$ direction. So, the inner product $x \\cdot w$ gives the **projection length**.\n",
        "\n",
        "The projection itself would be thus $x_w$ = $(x \\cdot w) w$.\n",
        "\n",
        "The product\n",
        "$X w$ **generalizes this concept**, projecting all rows (all data points) of\n",
        "$X$ onto the line defined by\n",
        "$w$.\n",
        "\n",
        "<img src=\"https://github.com/SzymonNowakowski/Machine-Learning-2024/blob/master/inner_product_as_projection_length.png?raw=1\" alt=\"inner product is a projection length\" width=\"400\" height=\"400\">"
      ],
      "metadata": {
        "id": "nV1wqLB0N3fu"
      },
      "id": "nV1wqLB0N3fu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How do we maximize $\\text{Var}(X w)$?\n",
        "\n",
        "\n",
        "\n",
        "To calculate the sample covariance matrix $C$ from our centered matrix $X$, we use the formula:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{n - 1} X^T X\n",
        "$$\n",
        "\n",
        "$C$ is the $ k \\times k $ sample covariance matrix. Since $C$ is symmetric, it can be decomposed as $C = V \\Lambda V^T$, where\n",
        "- $V$ is an orthonormal $ k \\times k $ matrix. Its columns are **eigenvectors** of $C$,\n",
        "- $\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_k)$ is a diagonal $ k \\times k $ matrix containing the **eigenvalues** of $C, \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_k$.\n",
        "\n",
        "Because $V$ is orthonormal, we have $V V^T = V^T V = I_k$, where $I_k$ is the identity matrix.\n",
        "\n",
        "Recall, we want to maximize:\n",
        "$$\n",
        "\\text{Var}(X w) \\longrightarrow \\max_{\\|w\\| = 1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Var}(X w)  = w^T C w = w^T V \\Lambda V^T w \\overset{\\omega = V^T w}{=} \\omega^T \\Lambda \\omega = \\sum_{i=1}^k \\lambda_i \\omega_i^2,\n",
        "$$\n",
        "where $\\| \\omega \\| = 1$.\n",
        "\n",
        "### The maximizer\n",
        "\n",
        "How do we maximize $\\text{Var}(X w)$? We choose $\\hat \\omega_i = (1, 0, 0, \\ldots, 0)^T$ as a maximizer.\n",
        "\n",
        "**Maximal variance** is then given by $\\lambda_1$.\n",
        "\n",
        "Now let's find a corresponding $\\hat w$.\n",
        "\n",
        "$\\hat w = I_k \\cdot \\hat w = V V^T \\hat w = V \\hat \\omega = V \\cdot (1, 0, 0, \\ldots, 0)^T = V_{\\cdot 1}$.  \n",
        "\n",
        "### The First Principal Component\n",
        "\n",
        "So the maximizer $\\hat w$, **the First Principal Component** of $X$, is the first column of $V$ (or **the first eigenvector** of $C$), with the **variance** given by its **first eigenvalue**.\n",
        "\n",
        "### The Next Principal Components\n",
        "It is not covered by this material, but the **next eigenvectors** of $C$ (or columns of $V$) are the next principal components with their **variance** given by their **corresponding eigenvalues**.\n",
        "\n"
      ],
      "metadata": {
        "id": "vBSv4GV5OAJ3"
      },
      "id": "vBSv4GV5OAJ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Singular Value Decomposition - full form\n",
        "\n",
        "Every $n \\times k$ matrix, $X$ included, can be decomposed as\n",
        "\n",
        "$$\n",
        "X= U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "- $U, V$ ortonormal matrices, $n\\times n$ and $k \\times k$, respectively\n",
        "- $\\Sigma$ is an $n \\times k$ matrix of diagonal **singular values** $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_{\\min(k,n)} \\geq 0$\n",
        "filled with additional zeros at the bottom or right:\n",
        "$$\n",
        "\\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\sigma_1 & 0 & \\dots & 0 \\\\\n",
        "0 & \\sigma_2 & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & \\sigma_k \\\\\n",
        "0 & 0 & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & 0 \\\\\n",
        "\\end{bmatrix} \\quad\\text{or}\\quad\n",
        "\\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\sigma_1 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "0 & \\sigma_2 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & \\sigma_n & 0 & \\dots & 0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In any case, $\\Sigma ^ 2 = \\Sigma^T \\Sigma$ is an $k \\times k$ diagonal matrix:\n",
        "\n",
        "$$\n",
        "\\Sigma ^ 2 = \\Sigma^T \\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\sigma_1^2 & 0 & \\dots & 0 \\\\\n",
        "0 & \\sigma_2^2 & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & \\sigma_k^2 \\\\\n",
        "\\end{bmatrix}\\quad \\text{or} \\quad\n",
        "\\Sigma ^ 2 = \\Sigma^T \\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\sigma_1^2 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "0 & \\sigma_2^2 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & \\sigma_n^2 & 0 & \\dots & 0 \\\\\n",
        "0 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**We shall work with the first case, as typically $n>k$.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "llqUHTosPbKI"
      },
      "id": "llqUHTosPbKI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Singular Value Decomposition - reduced form (used in practice, can be skipped now)\n",
        "\n",
        "Every $n \\times k$ matrix or rank $r$, $X$ included, can be decomposed as\n",
        "\n",
        "$$\n",
        "X= U_r \\Sigma_r V_r^T\n",
        "$$\n",
        "\n",
        "\n",
        "- $U_r$ has $r$ first columns from $U$ ($U_r$ is $n \\times r$). Its rows are truncated normalized vectors, thus they are not necessarily normalized. Thus, $U_r^T U_r = I_r$, but $U_r U_r^T \\ne I_n$ (in general).\n",
        "\n",
        "  Even for full rank $X$, usually $n>k$ and thus $n>r$ and thus $U_r \\ne U$.\n",
        "- $V_{r}$ has $r$ first columns from $V$ ($V_r$ is $k \\times r$). Its rows are truncated normalized vectors, thus they are not necessarily normalized. Thus, $V_r^T V_r = I_r$, but $V_r V_r^T \\ne I_k$ (in general).\n",
        "\n",
        "  However, for full rank $X$, $k=r$ (if we assume $n>k$ as is generally the case) and then $V_r$ = $V$.\n",
        "\n",
        "- $\\Sigma_{r}=\\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$ is a diagonal $ r \\times r $ matrix containing the **singular values** $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r > 0$."
      ],
      "metadata": {
        "id": "E1Ulka0FoHoU"
      },
      "id": "E1Ulka0FoHoU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Going back to full form SVD\n",
        "\n",
        "Recall,\n",
        "$$\n",
        "C = \\frac{1}{n - 1} X^T X\n",
        "$$\n",
        "but we want to have C decomposed as\n",
        "$$\n",
        "C = V \\Lambda V^T\n",
        "$$\n",
        "\n",
        "Now, using the SVD of X\n",
        "$$\n",
        "C = \\frac{1}{n - 1} V \\Sigma^T U^T U \\Sigma V^T = V \\frac{\\Sigma ^ 2}{n - 1} V^T\n",
        "$$\n",
        "\n",
        "Do they look similar?\n",
        "\n",
        "Recall, **we didn't need to calculate $C$ in this**. All was required was the SVD of $X$ to get both $V$ (principal components) and variance values $\\lambda_i = \\frac{\\sigma_i ^ 2}{n - 1}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "P9i-xciSOwl6"
      },
      "id": "P9i-xciSOwl6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55cfb727",
      "metadata": {
        "id": "55cfb727"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importing necessary libraries for data manipulation, visualization, and PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Setting visualization style\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}