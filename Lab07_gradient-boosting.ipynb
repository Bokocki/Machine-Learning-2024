{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab07_gradient-boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 7 - Gradient Boosting\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "-------------------------\n",
        "Boosting (and Gradient Boosting) is a powerful ensemble learning method that builds models sequentially to correct errors made by previous models. Unlike Random Forests, which construct trees independently and then aggregate their outputs, Boosting trains models in a stage-wise fashion, minimizing a loss function at each step.\n",
        "\n",
        "Since you have already studied **CART (Classification and Regression Trees)** and **Random Forests**, this chapter introduces Gradient Boosting as an advanced tree-based method that often outperforms Random Forests in predictive tasks."
      ],
      "metadata": {
        "id": "R9R5Fv3--rnE"
      },
      "id": "R9R5Fv3--rnE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: AdaBoost\n",
        "------------------\n",
        "\n",
        "Before we begin, let's review the **Adaptive Boosting (AdaBoost) algorithm** for classification.\n",
        "\n",
        "We assume a training dataset of $n$ samples, $\\{(x_i, y_i)\\}_{i=1}^{n}$, where $x_i \\in \\mathbb{R}^d$ are input features and $y_i \\in \\{-1, +1\\}$ are binary class labels.\n",
        "\n",
        "---\n",
        "\n",
        "## **Algorithm: AdaBoost**\n",
        "**Input**: Training data $\\{(x_i, y_i)\\}_{i=1}^{n}$, weak learner $h(x; \\theta)$, number of iterations $M$  \n",
        "**Output**: Final ensemble model $F_M(x)$  \n",
        "\n",
        "1. **Initialize** sample weights:  \n",
        "   $$ w_i^{(1)} = \\frac{1}{n}, \\quad \\forall i \\in \\{1, \\dots, n\\} $$\n",
        "\n",
        "2. **For** $m = 1$ to $M$ **do**:\n",
        "   1. Train weak classifier $h_m(x)$ using weighted dataset $\\{(x_i, y_i, w_i^{(m)})\\}$.\n",
        "\n",
        "   2. Compute weighted classification error:\n",
        "      $$ \\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i^{(m)} \\mathbb{1}(h_m(x_i) \\neq y_i)}{\\sum_{i=1}^{n} w_i^{(m)}} $$\n",
        "\n",
        "   3. Compute model weight:\n",
        "      $$ \\alpha_m = \\frac{1}{2} \\log \\frac{1 - \\epsilon_m}{\\epsilon_m} $$\n",
        "\n",
        "   4. Update sample weights:\n",
        "      $$ w_i^{(m+1)} = w_i^{(m)} \\exp\\left(-\\alpha_m y_i h_m(x_i) \\right) $$\n",
        "\n",
        "   5. Normalize weights:\n",
        "      $$ w_i^{(m+1)} = \\frac{w_i^{(m+1)}}{\\sum_{j=1}^{n} w_j^{(m+1)}} $$\n",
        "\n",
        "3. **Return** final classification model:\n",
        "   $$ F_M(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x) $$\n",
        "   $$ \\hat{y}(x) = \\text{sign}(F_M(x)) $$\n",
        "\n",
        "---\n",
        "\n",
        "## **Notes:**\n",
        "- The weak learner $h(x; \\theta)$ is typically a **stump** (a decision tree of depth 1).\n",
        "- AdaBoost assigns **higher weights to misclassified samples**, making future weak learners focus on harder cases.\n",
        "- The model weight $\\alpha_m$ determines how much influence each weak classifier has on the final prediction.\n",
        "- The final ensemble decision is based on the **weighted vote** of all weak classifiers.\n",
        "- Unlike standard regression boosting, AdaBoost uses **exponential reweighting** instead of residual fitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "Te9G0Oyh1nkI"
      },
      "id": "Te9G0Oyh1nkI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: Regression Boosting\n",
        "------------------\n",
        "\n",
        "Let's also review the boosting algorithm for regression.\n",
        "\n",
        "We assume a training dataset of $n$ samples, $\\{(x_i, y_i)\\}_{i=1}^{n}$, where $x_i \\in \\mathbb{R}^d$ are input features and $y_i \\in \\mathbb{R}$ are target values.\n"
      ],
      "metadata": {
        "id": "4TQINmWr0a9z"
      },
      "id": "4TQINmWr0a9z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Algorithm: Regression Boosting**\n",
        "**Input**: Training data $\\{(x_i, y_i)\\}_{i=1}^{n}$, weak learner $h(x; \\theta)$, number of iterations $M$  \n",
        "**Output**: Final ensemble model $F_M(x)$  \n",
        "\n",
        "1. **Initialize** the ensemble model with a constant value (e.g., the mean target value):\n",
        "   $$ F_0(x) = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\n",
        "\n",
        "2. **For** $m = 1$ to $M$ **do**:\n",
        "   1. Compute the residuals:\n",
        "      $$ r_i^{(m)} = y_i - F_{m-1}(x_i), \\quad \\forall i \\in \\{1, \\dots, n\\} $$\n",
        "\n",
        "   2. Fit a weak learner $h_m(x)$ to predict residuals:\n",
        "      \n",
        "      $$ h_m = \\arg\\min_{\\theta} \\sum_{i=1}^{n} \\left( r_i^{(m)} - h(x_i; \\theta) \\right)^2 $$\n",
        "\n",
        "   3. Update the ensemble model:\n",
        "      $$ F_m(x) = F_{m-1}(x) + \\lambda h_m(x) $$\n",
        "\n",
        "      where $\\lambda$ is a shrinkage parameter (learning rate) controlling the contribution of each weak learner.\n",
        "\n",
        "3. **Return** final model $F_M(x)$.\n",
        "\n",
        "---\n",
        "\n",
        "## **Notes:**\n",
        "- The weak learner $h(x; \\theta)$ is typically a shallow regression tree or another simple model.\n",
        "- This algorithm directly fits weak learners to residual errors.\n",
        "- The shrinkage parameter $\\lambda$ prevents overfitting by controlling the influence of each weak model.\n",
        "\n"
      ],
      "metadata": {
        "id": "iXHgLvMp0BZD"
      },
      "id": "iXHgLvMp0BZD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The Idea Behind Gradient Boosting\n",
        "\n",
        "Gradient Boosting builds an additive model of weak learners (typically decision trees) to minimize a given loss function. The model is updated iteratively:\n",
        "\n",
        "1. Start with an initial model, typically a constant value:\n",
        "\n",
        "   $$ F_0(x) = \\arg\\min_c \\sum_{i=1}^{n} L(y_i, c) $$\n",
        "\n",
        "   where $L(y_i, c)$ is the loss function (e.g., squared loss for regression or log loss for classification).\n",
        "\n",
        "2. For each iteration $m$, compute the residuals (pseudo-residuals):\n",
        "\n",
        "   $$ r_{i}^{(m)} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m-1}} $$\n",
        "\n",
        "   This represents the negative gradient of the loss function.\n",
        "\n",
        "3. Fit a new weak learner $h_m(x)$ (a shallow decision tree) to predict these residuals.\n",
        "\n",
        "4. Update the model:\n",
        "\n",
        "   $$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
        "\n",
        "   where $\\eta$ (learning rate) controls the contribution of each weak learner.\n"
      ],
      "metadata": {
        "id": "V8hfbiNR-wWF"
      },
      "id": "V8hfbiNR-wWF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Comparing Gradient Boosting to Random Forests\n",
        "\n",
        "| Feature              | Random Forests                          | Gradient Boosting                    |\n",
        "|----------------------|--------------------------------|--------------------------------|\n",
        "| Model Structure     | Ensemble of independent trees | Trees built sequentially       |\n",
        "| Training Process   | Uses bagging (parallel training) | Boosting (sequential corrections) |\n",
        "| Overfitting Risk   | Lower (averaging reduces variance) | Higher (but can be controlled) |\n",
        "| Performance        | Strong, but may not optimize loss | Often superior for complex tasks |\n",
        "| Speed             | Faster (can be parallelized) | Slower (sequential training) |\n"
      ],
      "metadata": {
        "id": "M7oa_Fxn-yHY"
      },
      "id": "M7oa_Fxn-yHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Hyperparameters in Gradient Boosting\n",
        "\n",
        "Tuning hyperparameters is crucial for Gradient Boosting performance. Key parameters include:\n",
        "\n",
        "- **Learning Rate $\\eta$**: Controls how much each tree contributes. Small values (e.g., 0.01) require more trees.\n",
        "- **Number of Trees $M$**: Too many trees may lead to overfitting.\n",
        "- **Tree Depth $d$**: Controls the complexity of each tree. Shallow trees (e.g., depth = 3-5) work well.\n",
        "- **Min Samples Split & Min Samples Leaf**: Define when to stop growing trees.\n",
        "- **Subsample**: Fraction of data used to train each tree, introducing randomness (like in Random Forests).\n",
        "\n",
        "\n",
        "\n",
        "In the next section, we will implement Gradient Boosting in Python using `scikit-learn` and `XGBoost`.\n"
      ],
      "metadata": {
        "id": "Pf14NK1d9m5p"
      },
      "id": "Pf14NK1d9m5p"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}