{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab07_gradient-boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 7 - Gradient Boosting\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "-------------------------\n",
        "Gradient Boosting is a powerful ensemble learning method that builds models sequentially to correct errors made by previous models. Unlike Random Forests, which construct trees independently and then aggregate their outputs, Gradient Boosting trains models in a stage-wise fashion, minimizing a loss function at each step.\n",
        "\n",
        "Since you have already studied **CART (Classification and Regression Trees)** and **Random Forests**, this chapter introduces Gradient Boosting as an advanced tree-based method that often outperforms Random Forests in predictive tasks."
      ],
      "metadata": {
        "id": "R9R5Fv3--rnE"
      },
      "id": "R9R5Fv3--rnE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The Idea Behind Gradient Boosting\n",
        "\n",
        "Gradient Boosting builds an additive model of weak learners (typically decision trees) to minimize a given loss function. The model is updated iteratively:\n",
        "\n",
        "1. Start with an initial model, typically a constant value:\n",
        "\n",
        "   $$ F_0(x) = \\arg\\min_c \\sum_{i=1}^{n} L(y_i, c) $$\n",
        "\n",
        "   where $L(y_i, c)$ is the loss function (e.g., squared loss for regression or log loss for classification).\n",
        "\n",
        "2. For each iteration $m$, compute the residuals (pseudo-residuals):\n",
        "\n",
        "   $$ r_{i}^{(m)} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m-1}} $$\n",
        "\n",
        "   This represents the negative gradient of the loss function.\n",
        "\n",
        "3. Fit a new weak learner $h_m(x)$ (a shallow decision tree) to predict these residuals.\n",
        "\n",
        "4. Update the model:\n",
        "\n",
        "   $$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
        "\n",
        "   where $\\eta$ (learning rate) controls the contribution of each weak learner.\n"
      ],
      "metadata": {
        "id": "V8hfbiNR-wWF"
      },
      "id": "V8hfbiNR-wWF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Comparing Gradient Boosting to Random Forests\n",
        "\n",
        "| Feature              | Random Forests                          | Gradient Boosting                    |\n",
        "|----------------------|--------------------------------|--------------------------------|\n",
        "| Model Structure     | Ensemble of independent trees | Trees built sequentially       |\n",
        "| Training Process   | Uses bagging (parallel training) | Boosting (sequential corrections) |\n",
        "| Overfitting Risk   | Lower (averaging reduces variance) | Higher (but can be controlled) |\n",
        "| Performance        | Strong, but may not optimize loss | Often superior for complex tasks |\n",
        "| Speed             | Faster (can be parallelized) | Slower (sequential training) |\n"
      ],
      "metadata": {
        "id": "M7oa_Fxn-yHY"
      },
      "id": "M7oa_Fxn-yHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Hyperparameters in Gradient Boosting\n",
        "\n",
        "Tuning hyperparameters is crucial for Gradient Boosting performance. Key parameters include:\n",
        "\n",
        "- **Learning Rate $\\eta$**: Controls how much each tree contributes. Small values (e.g., 0.01) require more trees.\n",
        "- **Number of Trees $M$**: Too many trees may lead to overfitting.\n",
        "- **Tree Depth $d$**: Controls the complexity of each tree. Shallow trees (e.g., depth = 3-5) work well.\n",
        "- **Min Samples Split & Min Samples Leaf**: Define when to stop growing trees.\n",
        "- **Subsample**: Fraction of data used to train each tree, introducing randomness (like in Random Forests).\n",
        "\n",
        "\n",
        "\n",
        "In the next section, we will implement Gradient Boosting in Python using `scikit-learn` and `XGBoost`.\n"
      ],
      "metadata": {
        "id": "Pf14NK1d9m5p"
      },
      "id": "Pf14NK1d9m5p"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}