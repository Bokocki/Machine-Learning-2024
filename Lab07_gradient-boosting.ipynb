{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab07_gradient-boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 7 - Gradient Boosting\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "-------------------------\n",
        "Boosting (and Gradient Boosting) is a powerful ensemble learning method that builds models sequentially to correct errors made by previous models. Unlike Random Forests, which construct trees independently and then aggregate their outputs, Boosting trains models in a stage-wise fashion, minimizing a loss function at each step.\n",
        "\n",
        "Since you have already studied **CART (Classification and Regression Trees)** and **Random Forests**, this chapter introduces Gradient Boosting as an advanced tree-based method that often outperforms Random Forests in predictive tasks."
      ],
      "metadata": {
        "id": "R9R5Fv3--rnE"
      },
      "id": "R9R5Fv3--rnE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: AdaBoost\n",
        "------------------\n",
        "\n",
        "Before we begin, let's review the **Adaptive Boosting (AdaBoost) algorithm** for classification.\n",
        "\n",
        "We assume a training dataset of $n$ samples, $\\{(x_i, y_i)\\}_{i=1}^{n}$, where $x_i \\in \\mathbb{R}^d$ are input features and $y_i \\in \\{-1, +1\\}$ are binary class labels.\n",
        "\n",
        "---\n",
        "\n",
        "## **Algorithm: AdaBoost**\n",
        "**Input**: Training data $\\{(x_i, y_i)\\}_{i=1}^{n}$, weak learner $h(x; \\theta)$, number of iterations $M$  \n",
        "**Output**: Final ensemble model $F_M(x)$  \n",
        "\n",
        "1. **Initialize** sample weights:  \n",
        "   $$ w_i^{(1)} = \\frac{1}{n}, \\quad \\forall i \\in \\{1, \\dots, n\\} $$\n",
        "\n",
        "2. **For** $m = 1$ to $M$ **do**:\n",
        "   1. Train weak classifier $h_m(x)$ using weighted dataset $\\{(x_i, y_i, w_i^{(m)})\\}$.\n",
        "\n",
        "   2. Compute weighted classification error:\n",
        "      $$ \\epsilon_m = \\frac{\\sum_{i=1}^{n} w_i^{(m)} \\mathbb{1}(h_m(x_i) \\neq y_i)}{\\sum_{i=1}^{n} w_i^{(m)}} $$\n",
        "\n",
        "   3. Compute model weight:\n",
        "      $$ \\alpha_m = \\frac{1}{2} \\log \\frac{1 - \\epsilon_m}{\\epsilon_m} $$\n",
        "\n",
        "   4. Update sample weights:\n",
        "      $$ w_i^{(m+1)} = w_i^{(m)} \\exp\\left(-\\alpha_m y_i h_m(x_i) \\right) $$\n",
        "\n",
        "   5. Normalize weights:\n",
        "      $$ w_i^{(m+1)} = \\frac{w_i^{(m+1)}}{\\sum_{j=1}^{n} w_j^{(m+1)}} $$\n",
        "\n",
        "3. **Return** final classification model:\n",
        "   $$ F_M(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x) $$\n",
        "   $$ \\hat{y}(x) = \\text{sign}(F_M(x)) $$\n",
        "\n",
        "---\n",
        "\n",
        "## **Notes:**\n",
        "- The weak learner $h(x; \\theta)$ is typically a **stump** (a decision tree of depth 1).\n",
        "- AdaBoost assigns **higher weights to misclassified samples**, making future weak learners focus on harder cases.\n",
        "- The model weight $\\alpha_m$ determines how much influence each weak classifier has on the final prediction.\n",
        "- The final ensemble decision is based on the **weighted vote** of all weak classifiers.\n",
        "- Unlike standard regression boosting, AdaBoost uses **exponential reweighting** instead of residual fitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "Te9G0Oyh1nkI"
      },
      "id": "Te9G0Oyh1nkI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: Regression Boosting\n",
        "------------------\n",
        "\n",
        "Let's also review the boosting algorithm for regression.\n",
        "\n",
        "We assume a training dataset of $n$ samples, $\\{(x_i, y_i)\\}_{i=1}^{n}$, where $x_i \\in \\mathbb{R}^d$ are input features and $y_i \\in \\mathbb{R}$ are target values.\n"
      ],
      "metadata": {
        "id": "4TQINmWr0a9z"
      },
      "id": "4TQINmWr0a9z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Algorithm: Regression Boosting**\n",
        "**Input**: Training data $\\{(x_i, y_i)\\}_{i=1}^{n}$, weak learner $h(x; \\theta)$, number of iterations $M$  \n",
        "**Output**: Final ensemble model $F_M(x)$  \n",
        "\n",
        "1. **Initialize** the ensemble model with a constant value (e.g., the mean target value):\n",
        "   $$ F_0(x) = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\n",
        "\n",
        "2. **For** $m = 1$ to $M$ **do**:\n",
        "   1. Compute the residuals:\n",
        "      $$ r_i^{(m)} = y_i - F_{m-1}(x_i), \\quad \\forall i \\in \\{1, \\dots, n\\} $$\n",
        "\n",
        "   2. Fit a weak learner $h_m(x)$ to predict residuals:\n",
        "      \n",
        "      $$ h_m = \\arg\\min_{\\theta} \\sum_{i=1}^{n} \\left( r_i^{(m)} - h(x_i; \\theta) \\right)^2 $$\n",
        "\n",
        "   3. Update the ensemble model:\n",
        "      $$ F_m(x) = F_{m-1}(x) + \\lambda h_m(x) $$\n",
        "\n",
        "      where $\\lambda$ is a shrinkage parameter (learning rate) controlling the contribution of each weak learner.\n",
        "\n",
        "3. **Return** final model $F_M(x)$.\n",
        "\n",
        "---\n",
        "\n",
        "## **Notes:**\n",
        "- The weak learner $h(x; \\theta)$ is typically a shallow regression tree or another simple model.\n",
        "- This algorithm directly fits weak learners to residual errors.\n",
        "- The shrinkage parameter $\\lambda$ prevents overfitting by controlling the influence of each weak model.\n",
        "\n"
      ],
      "metadata": {
        "id": "iXHgLvMp0BZD"
      },
      "id": "iXHgLvMp0BZD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward Stagewise Additive Modelling (FSAM)\n",
        "------------------\n",
        "\n",
        "At first glance, the algorithms for **Regression Boosting** and **AdaBoost** appear quite different. However, both can be seen as instances of a **more general framework**: **Forward Stagewise Additive Modelling (FSAM)**.\n",
        "\n",
        "FSAM builds a function $F(x)$ incrementally by minimizing a loss function $L(y, F(x))$ in a stagewise fashion."
      ],
      "metadata": {
        "id": "vgDcDPFxAGXn"
      },
      "id": "vgDcDPFxAGXn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Algorithm: Forward Stagewise Additive Modeling**\n",
        "\n",
        "\n",
        "**Input**: Training data $\\{(x_i, y_i)\\}_{i=1}^{N}$, number of iterations $M$\n",
        "\n",
        "**Output**: Estimated function $\\hat{f}(x)$\n",
        "\n",
        "1. **Initialize** the function estimate:\n",
        "   $$\n",
        "   f_0(x) = 0\n",
        "   $$\n",
        "\n",
        "2. **For** $m = 1$ to $M$:\n",
        "   - (a) Compute:\n",
        "     $$\n",
        "     (\\beta_m, \\gamma_m) = \\arg\\min_{\\beta, \\gamma} \\sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + \\beta b(x_i; \\gamma))\n",
        "     $$\n",
        "   - (b) Update the function estimate:\n",
        "     $$\n",
        "     f_m(x) = f_{m-1}(x) + \\beta_m b(x; \\gamma_m)\n",
        "     $$\n",
        "\n",
        "3. **Output** the final model:\n",
        "   $$\n",
        "   \\hat{f}(x) = f_M(x)\n",
        "   $$\n",
        "\n",
        "**Notes**:\n",
        "- The basis functions $b(x; \\gamma)$ are typically simple functions (e.g., decision stumps or trees) parameterized by $\\gamma$.\n",
        "- The algorithm incrementally builds the model by adding new basis functions at each iteration to minimize the loss function $L(y, f(x))$.\n",
        "- The choice of loss function $L$ influences the behavior and robustness of the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lsRofp2T8qOd"
      },
      "id": "lsRofp2T8qOd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Role of $\\gamma$ (Structure Parameter)\n",
        "\n",
        "$\\gamma$ parametrizes the basis function $b(x; \\gamma)$.  \n",
        "It determines the structure or specific form of the weak learner being added.\n",
        "\n",
        "**Example:**  \n",
        "- If $b(x; \\gamma)$ is a **decision tree**, then $\\gamma$ could represent:\n",
        "  - The split point(s) of the tree.\n",
        "  - The features chosen for splitting.\n",
        "  - The depth of the tree.\n",
        "- If $b(x; \\gamma)$ is a **linear function**, then $\\gamma$ could be:\n",
        "  - The slope and intercept of the line.\n"
      ],
      "metadata": {
        "id": "nK0vydxaACBB"
      },
      "id": "nK0vydxaACBB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Role of $\\beta$ (Scaling Parameter)\n",
        "\n",
        "$\\beta$ determines how much influence the new basis function $b(x; \\gamma)$ has in the updated model.  \n",
        "It acts as a coefficient that controls the step size in the direction of the selected weak learner.\n",
        "\n",
        "**Example:**  \n",
        "If we are minimizing squared error, $\\beta$ could be chosen as:\n",
        "\n",
        "$$\n",
        "\\beta_m = \\arg\\min_{\\beta} \\sum_{i=1}^{N} \\left( y_i - ( f_{m-1}(x_i) + \\beta b(x_i; \\gamma_m) ) \\right)^2\n",
        "$$\n",
        "\n",
        "which means we find $\\beta$ that minimizes the squared error after adding the weak learner.\n",
        "\n",
        "If $b(x; \\gamma)$ is a **decision tree predicting residuals**, $\\beta$ **scales the tree’s predictions** before adding it to the ensemble.\n",
        "\n",
        "Thus, $\\beta$ controls the **contribution (weight) of each new weak learner** in the final boosted model.\n",
        "With $\\beta = 1.0$, the model would assume that the weak learner's predictions are **perfectly scaled** to correct the residual errors. However, this is rarely the case."
      ],
      "metadata": {
        "id": "SggK-pRaAv1j"
      },
      "id": "SggK-pRaAv1j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connection of FSAM and other Boosting Techniques\n",
        "\n",
        "📚 **See also** → *The Elements of Statistical Learning*:  \n",
        "Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer, 2009.  \n",
        "[📖 Link to the book](https://hastie.su.domains/ElemStatLearn/)\n",
        "(Chapter 10.3)\n",
        "\n",
        "For the squared loss function:\n",
        "\n",
        "$$\n",
        "L(y,F(x))=(y−F(x))^2\n",
        "$$\n",
        "\n",
        "substituting this into the given equation gives:\n",
        "\n",
        "$$\n",
        "(\\beta_m, \\gamma_m) = \\arg\\min_{\\beta, \\gamma} \\sum_{i=1}^{N} (y_i - (f_{m-1}(x_i) + \\beta b(x_i; \\gamma)))^2\n",
        "$$\n",
        "\n",
        "Now, rewriting in terms of residuals:\n",
        "\n",
        "$$\n",
        "r_i^{(m)} = y_i - f_{m-1}(x_i)\n",
        "$$\n",
        "\n",
        "Substituting $r_i^{(m)}$ into the equation:\n",
        "\n",
        "$$\n",
        "(\\beta_m, \\gamma_m) = \\arg\\min_{\\beta, \\gamma} \\sum_{i=1}^{N} (r_i^{(m)} - \\beta b(x_i; \\gamma))^2\n",
        "$$\n",
        "\n",
        "This shows that in the case of squared loss, the optimization problem in Forward Stagewise Additive Modeling naturally decomposes into residual fitting which (modulo shrinkage parameters) is in essence Regression Boosting.\n",
        "\n",
        "Similarly, we can show that AdaBoost is the Forward Stagewise Additive Modeling algorithm with exponential loss $L(y, F(x)) = e^{-yF(x)}$.\n"
      ],
      "metadata": {
        "id": "HUOo-yEeCIJq"
      },
      "id": "HUOo-yEeCIJq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The Idea Behind Gradient Boosting\n",
        "\n",
        "Gradient Boosting builds an additive model of weak learners (typically decision trees) to minimize a given loss function. The model is updated iteratively:\n",
        "\n",
        "1. Start with an initial model, typically a constant value:\n",
        "\n",
        "   $$ F_0(x) = \\arg\\min_c \\sum_{i=1}^{n} L(y_i, c) $$\n",
        "\n",
        "   where $L(y_i, c)$ is the loss function (e.g., squared loss for regression or log loss for classification).\n",
        "\n",
        "2. For each iteration $m$, compute the residuals (pseudo-residuals):\n",
        "\n",
        "   $$ r_{i}^{(m)} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m-1}} $$\n",
        "\n",
        "   This represents the negative gradient of the loss function.\n",
        "\n",
        "3. Fit a new weak learner $h_m(x)$ (a shallow decision tree) to predict these residuals.\n",
        "\n",
        "4. Update the model:\n",
        "\n",
        "   $$ F_m(x) = F_{m-1}(x) + \\eta h_m(x) $$\n",
        "\n",
        "   where $\\eta$ (learning rate) controls the contribution of each weak learner.\n"
      ],
      "metadata": {
        "id": "V8hfbiNR-wWF"
      },
      "id": "V8hfbiNR-wWF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Comparing Gradient Boosting to Random Forests\n",
        "\n",
        "| Feature              | Random Forests                          | Gradient Boosting                    |\n",
        "|----------------------|--------------------------------|--------------------------------|\n",
        "| Model Structure     | Ensemble of independent trees | Trees built sequentially       |\n",
        "| Training Process   | Uses bagging (parallel training) | Boosting (sequential corrections) |\n",
        "| Overfitting Risk   | Lower (averaging reduces variance) | Higher (but can be controlled) |\n",
        "| Performance        | Strong, but may not optimize loss | Often superior for complex tasks |\n",
        "| Speed             | Faster (can be parallelized) | Slower (sequential training) |\n"
      ],
      "metadata": {
        "id": "M7oa_Fxn-yHY"
      },
      "id": "M7oa_Fxn-yHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Hyperparameters in Gradient Boosting\n",
        "\n",
        "Tuning hyperparameters is crucial for Gradient Boosting performance. Key parameters include:\n",
        "\n",
        "- **Learning Rate $\\eta$**: Controls how much each tree contributes. Small values (e.g., 0.01) require more trees.\n",
        "- **Number of Trees $M$**: Too many trees may lead to overfitting.\n",
        "- **Tree Depth $d$**: Controls the complexity of each tree. Shallow trees (e.g., depth = 3-5) work well.\n",
        "- **Min Samples Split & Min Samples Leaf**: Define when to stop growing trees.\n",
        "- **Subsample**: Fraction of data used to train each tree, introducing randomness (like in Random Forests).\n",
        "\n",
        "\n",
        "\n",
        "In the next section, we will implement Gradient Boosting in Python using `scikit-learn` and `XGBoost`.\n"
      ],
      "metadata": {
        "id": "Pf14NK1d9m5p"
      },
      "id": "Pf14NK1d9m5p"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}