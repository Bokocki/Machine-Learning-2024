{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab04_penalized-regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab4 - Penalized Regression\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Penalization Methods\n",
        "-----------------------\n",
        "\n",
        "In many statistical and machine learning problems, overfitting arises when the model learns noise instead of the underlying patterns in the data. Penalization methods address this issue by adding a penalty term to the objective function, discouraging overly complex models. This results in solutions that are simpler and generalize better to unseen data. The rationale for penalization methods lies in controlling the complexity of the model to balance the trade-off between bias and variance. Penalization is also key in feature selection and regularization, where the goal is to identify relevant predictors while mitigating multicollinearity and stabilizing parameter estimates."
      ],
      "metadata": {
        "id": "B1K2cKMGKi0Z"
      },
      "id": "B1K2cKMGKi0Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The $\\ell_0$ Penalty: Variable Selection\n",
        "\n",
        "The penalized objective function for $\\ell_0$ regression is:\n",
        "\n",
        "$$\n",
        "\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_0,\n",
        "$$\n",
        "\n",
        "The $\\ell_0$ penalty is defined as the count of non-zero coefficients in the regression model:\n",
        "$$\n",
        "\\|\\beta\\|_0 = \\sum_{j=1}^p \\mathbb{1}(\\beta_j \\neq 0),\n",
        "$$\n",
        "where $\\beta_j$ is the $j$-th coefficient, and $\\mathbb{1}(\\cdot)$ is the indicator function and $\\lambda > 0$ controls the penalty strength.\n",
        "The $\\ell_0$ penalty directly enforces sparsity by minimizing the number of non-zero coefficients, effectively performing variable selection. However, the $\\ell_0$ problem is computationally intractable because it requires solving a combinatorial optimization problem, which is NP-hard. For this reason, alternatives like $\\ell_1$ penalization are commonly used.\n",
        "\n",
        "Still,\n",
        "$\\ell_0$-based variable selection remains a **conceptual benchmark**: it precisely encodes the goal of picking as few features as possible while minimizing the residual sum of squares.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0wwxLEWkx2Eb"
      },
      "id": "0wwxLEWkx2Eb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The $\\ell_1$ Penalty: Lasso\n",
        "The Lasso (Least Absolute Shrinkage and Selection Operator) method uses the $\\ell_1$ norm as the penalty term.\n",
        "\n",
        "The penalized objective function for Lasso regression is:\n",
        "$$\n",
        "\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1,\n",
        "$$\n",
        "with\n",
        "$$\n",
        "\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|.\n",
        "$$\n",
        "where $\\lambda > 0$ controls the penalty strength. The $\\ell_1$ penalty induces sparsity by shrinking some coefficients exactly to zero, making it effective for variable selection in high-dimensional settings.\n"
      ],
      "metadata": {
        "id": "z6jVlb_Bzcd3"
      },
      "id": "z6jVlb_Bzcd3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The $\\ell_2$ Penalty: Ridge Regression\n",
        "Ridge regression uses the $\\ell_2$ norm as the penalty:\n",
        "$$\n",
        "\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2.\n",
        "$$\n",
        "The penalized objective function for ridge regression is:\n",
        "$$\n",
        "\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2.\n",
        "$$ with $\\lambda > 0$ controlling the penalty strength.\n",
        "\n",
        "Unlike Lasso, ridge regression does not induce sparsity but shrinks all coefficients toward zero, making it effective for handling multicollinearity and stabilizing the model when predictors are highly correlated.\n",
        "\n"
      ],
      "metadata": {
        "id": "q3NEC8iv0c4z"
      },
      "id": "q3NEC8iv0c4z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manifold Perspective for Lasso and Ridge\n",
        "\n",
        "Both the Lasso and Ridge regression objectives can be interpreted as optimizing the unpenalized sum of squares subject to a constraint (a “manifold”) determined by the penalty. This constrained problem can be solved using the method of Lagrange multipliers, which connects the penalized and constrained formulations. The key difference lies in the shape of the constraint region and how it interacts with the **level curves** (contours) of the unpenalized sum of squares."
      ],
      "metadata": {
        "id": "v7S1WPDsKk6L"
      },
      "id": "v7S1WPDsKk6L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ridge Regression\n",
        "\n",
        "For Ridge regression, the penalized objective is:\n",
        "$$\n",
        "\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2.\n",
        "$$\n",
        "Alternatively, it can be written as a constrained optimization problem:\n",
        "$$\n",
        "\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\leq c,\n",
        "$$\n",
        "where $c$ is the radius of an $\\ell_2$ ball.\n",
        "\n",
        "The Lagrange multiplier $\\lambda$ controls the strength of the penalty and relates to $c$ as follows:\n",
        "- $\\lambda$ determines how tightly the solution is constrained.\n",
        "- Increasing $\\lambda$ corresponds to decreasing $c$, shrinking the radius of the feasible region.\n",
        "\n",
        "In Ridge regression, the feasible region $\\{\\beta : \\|\\beta\\|_2^2 \\leq c\\}$ is a smooth sphere. The optimal solution occurs where the **level curves** (elliptical contours) of the unpenalized sum of squares first touch the sphere. Because the sphere is smooth, this contact rarely happens at a coordinate axis, so Ridge regression typically shrinks coefficients but does not force them to exactly zero."
      ],
      "metadata": {
        "id": "8J_RNx2UKnfu"
      },
      "id": "8J_RNx2UKnfu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso\n",
        "\n",
        "For Lasso regression, the penalized objective is:\n",
        "$$\n",
        "\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1.\n",
        "$$\n",
        "Alternatively, it can be written as:\n",
        "$$\n",
        "\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_1 \\leq c,\n",
        "$$\n",
        "where $c$ is the radius of an $\\ell_1$ ball.\n",
        "\n",
        "The Lagrange multiplier $\\lambda$ and the ball radius $c$ are related similarly to Ridge:\n",
        "- $\\lambda$ controls the strength of the penalty.\n",
        "- Larger $\\lambda$ corresponds to a smaller $c$, tightening the constraint.\n",
        "\n",
        "For Lasso, the feasible region $\\{\\beta : \\|\\beta\\|_1 \\leq c\\}$ is an $\\ell_1$ ball, which is a polytope shaped like a diamond or octahedron. The sharp corners of the polytope lie on coordinate axes. The solution occurs where the **level curves** of the unpenalized sum of squares first touch the polytope. Because the first contact often occurs at a corner, Lasso tends to shrink some coefficients exactly to zero, producing a sparse solution."
      ],
      "metadata": {
        "id": "0xt3tftsKps-"
      },
      "id": "0xt3tftsKps-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Relationship Between $\\lambda$ and $c$\n",
        "\n",
        "In both Lasso and Ridge, $\\lambda$ and $c$ are not equal but are related through the optimization problem. Specifically:\n",
        "- $\\lambda$ appears in the penalized formulation and determines the relative importance of the penalty term.\n",
        "- $c$ defines the size of\n"
      ],
      "metadata": {
        "id": "W5rb-av12CiA"
      },
      "id": "W5rb-av12CiA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Lasso\n",
        "The **group Lasso** extends Lasso to encourage **groupwise sparsity**. Suppose the parameters $\\beta$ are partitioned into groups $\\{G_1, G_2, \\dots\\}$. Then we impose:\n",
        "\n",
        "$$\n",
        "\\sum_{g} \\|\\beta_{G_g}\\|_2\n",
        "$$\n",
        "\n",
        "as the penalty, where $\\beta_{G_g}$ are the coefficients for group $g$. Minimizing:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^n \\left(y_i - x_i^\\top \\beta\\right)^2 + \\lambda \\sum_{g} \\|\\beta_{G_g}\\|_2.\n",
        "$$\n",
        "\n",
        "\n",
        "The penalty $\\sum_{g} \\|\\beta_{G_g}\\|_2$ combines two behaviors:\n",
        "1. **Between-group penalty**: The $\\ell_1$-like behavior of the penalty $\\|\\beta_{G_g}\\|_2$ enforces sparsity at the group level, meaning that entire groups of coefficients can be zeroed out if their contribution to the objective is small.\n",
        "2. **Within-group penalty**: The $\\ell_2$ norm within each group $\\|\\beta_{G_g}\\|_2$ applies Ridge-like shrinkage, ensuring smooth and proportional coefficient shrinkage within a group.\n",
        "\n",
        "If groups are irrelevant, the $\\ell_1$ penalty zeros out all coefficients for those groups. For groups retained in the solution, the $\\ell_2$ penalty prevents individual coefficients within the group from being arbitrarily zeroed out, providing a balance between group selection and regularization.\n",
        "\n",
        "Concrete examples include selecting entire subsets of correlated predictors in finance (grouping by sector) or picking all polynomial terms of a given variable together in polynomial expansions.\n",
        "\n"
      ],
      "metadata": {
        "id": "6__WZO8Y01VP"
      },
      "id": "6__WZO8Y01VP"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}